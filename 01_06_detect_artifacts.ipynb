{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artefact detection\n",
    "Roughly following the ideas about artifact detection outlined by Leo in prep_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/media/Linux6_Data/DATA/SFB2\"\n",
    "proc_dir = join(root_dir, \"proc\") # working directory\n",
    "\n",
    "sub_dirs = [item for item in listdir(proc_dir) if isdir(join(proc_dir, item))]\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "testing = True # for testing, only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_grad(raw, thresh=None, extend = 0.2, channel= None, start= 0, stop = None):\n",
    "    \"\"\"\n",
    "    Find and annotate gradient artifacts within the indices range (start, stop)\n",
    "    \"\"\"\n",
    "    data = raw.get_data(return_times=True, picks = channel, start= start, stop=stop)\n",
    "    times = data[1]\n",
    "    if channel:\n",
    "        channel_list = channel\n",
    "    else:\n",
    "        channel_list = raw.ch_names\n",
    "    annot_dict = dict(onset= list(), duration=list(), description = list(), orig_time = list(), ch_names = list())\n",
    "\n",
    "    pend = np.zeros((len(channel_list),1))\n",
    "    \n",
    "    derive = np.diff(data[0], axis=1, prepend=pend, append= pend)[:,0:-1]\n",
    "    if not derive.any():\n",
    "        return None\n",
    "\n",
    "    # if no threshold specified, try to guess one\n",
    "    if thresh==None: \n",
    "        median = np.median(derive, axis=1)\n",
    "        firstq, thirdq = np.percentile(derive, [25, 75], axis = 1)\n",
    "        interquar = thirdq - firstq\n",
    "        thresh = median + 20*interquar\n",
    "\n",
    "    \n",
    "    mask = np.zeros(np.shape(derive))\n",
    "    mask = np.where(abs(derive)>thresh[:, None], 1, mask)\n",
    "    segm = np.diff(mask, axis=1, prepend = pend, append = pend)[:,0:-1]\n",
    "\n",
    "    onsets = np.argwhere(segm == 1)\n",
    "    offsets = np.argwhere(segm == -1)\n",
    "    if len(onsets)!=len(offsets):\n",
    "        print(np.shape(segm))\n",
    "    print(len(onsets),len(offsets))\n",
    "\n",
    "    new_on = [[] for _ in range(len(channel_list))]\n",
    "    new_off = [[] for _ in range(len(channel_list))]\n",
    "    \n",
    "    for i in range(len(onsets)):\n",
    "        new_on[onsets[i][0]].append(onsets[i][1])\n",
    "    for i in range(len(offsets)):\n",
    "        new_off[offsets[i][0]].append(offsets[i][1])\n",
    "\n",
    "    for i, chan in enumerate(channel_list):\n",
    "        if len(new_on[i]) > len(new_off[i]):\n",
    "            new_off[i].append(new_on[i][-1])\n",
    "        annot_dict['onset']+=list(times[new_on[i]]-extend)\n",
    "        annot_dict['duration']+=list(times[new_off[i]]-times[new_on[i]]+2*extend)\n",
    "        annot_dict['description']+=list(np.repeat('BAD_step_'+chan, len(new_on[i])))\n",
    "        annot_dict['orig_time']+=list(np.repeat(raw.info['meas_date'], len(new_on[i])))\n",
    "        annot_dict['ch_names']+=list(np.tile(channel_list[i], (len(new_on[i]),1)))\n",
    "    for key, value in annot_dict.items():\n",
    "        annot_dict[key] = np.asarray(value)\n",
    "    annot = mne.Annotations(onset= annot_dict['onset'], duration= annot_dict['duration'], description= annot_dict['description'], orig_time = raw.info['meas_date'])\n",
    "\n",
    "    return annot\n",
    "\n",
    "\n",
    "\n",
    "def annot_abs(raw, thresh= 7.5e-4, extend = 0.2, channel= None, start = 0, stop = None):\n",
    "    \"\"\"\n",
    "    Find and annotate amplitude artifacts within the indices range (start, stop)\n",
    "    \"\"\"\n",
    "\n",
    "    data = raw.get_data(return_times=True, picks = channel, start = start, stop= stop)\n",
    "    times = data[1]\n",
    "    if channel:\n",
    "        channel_list = channel\n",
    "    else:\n",
    "        channel_list = raw.ch_names\n",
    "    bad_ints = dict(onset= list(), duration=list(), description = list(), orig_time = list(), ch_names = list())\n",
    "    \n",
    "\n",
    "    pend = np.zeros((len(channel_list),1))\n",
    "    mask = np.zeros(np.shape(data[0]))\n",
    "    mask = np.where(abs(data[0])>thresh, 1, mask)\n",
    "    segm = np.diff(mask, axis=1, prepend= pend, append=pend)[:,0:-1]\n",
    "    onsets = np.argwhere(segm == 1)\n",
    "    offsets = np.argwhere(segm == -1)\n",
    "    new_on = [[] for _ in range(len(channel_list))]\n",
    "    new_off = [[] for _ in range(len(channel_list))]\n",
    "    for i in range(len(onsets)):\n",
    "        new_on[onsets[i][0]].append(onsets[i][1])\n",
    "    for i in range(len(offsets)):\n",
    "        new_off[offsets[i][0]].append(offsets[i][1])\n",
    "    for i, chan in enumerate(channel_list):\n",
    "        if len(new_on[i]) > len(new_off[i]):\n",
    "            new_off[i].append(new_on[i][-1])\n",
    "        bad_ints['onset']+=list(times[new_on[i]]-extend)\n",
    "        bad_ints['duration']+=list(times[new_off[i]]-times[new_on[i]]+2*extend)\n",
    "        bad_ints['description']+=list(np.repeat('BAD_amplitude_'+chan, len(new_on[i])))\n",
    "        bad_ints['orig_time']+= list(np.repeat(raw.info['meas_date'], len(new_on[i])))\n",
    "        bad_ints['ch_names']+=list(np.tile(channel_list[i], (len(new_on[i]),1)))\n",
    "    for key, value in bad_ints.items():\n",
    "        bad_ints[key] = np.asarray(value)\n",
    "    annot= mne.Annotations(onset= bad_ints['onset'], duration= bad_ints['duration'], description= bad_ints['description'], ch_names = bad_ints['ch_names'], orig_time = raw.info['meas_date'])\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervals_from_annotations(raw, description, end_interval = True):\n",
    "    \"\"\"\n",
    "    Returns intervals (start, stop) indices of annotations based on an expression of annotation descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "        The raw data\n",
    "    description : list of str\n",
    "        The descriptions of the annotations to look for (use regex notation, e.g. ending with \".*\" to avoid specifying each individually)\n",
    "    end_interval : bool\n",
    "        if true, we add an interval from the last annotation index matching the description to the end of the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples (start int, end int, description str)\n",
    "    \"\"\"\n",
    "    times = raw.get_data(return_times = True)[1]\n",
    "    annotations = raw.annotations.copy()\n",
    "\n",
    "    annotation_intervals = []\n",
    "    # cycle through all annotations and find the ones that match elements in the description list\n",
    "    for annot in annotations:\n",
    "        for j in range(len(description)):\n",
    "            if re.match(description[j], annot['description']):\n",
    "                indices = raw.time_as_index([annot['onset'], annot['onset'] + annot['duration']])\n",
    "                annotation_intervals.append((indices[0], indices[1], annot['description']))\n",
    "    \n",
    "    if end_interval:\n",
    "        annotation_intervals.append((annotation_intervals[-1][1]+1, len(times)-1, 'End'))\n",
    "    return annotation_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_preprocessed_files = 0\n",
    "\n",
    "for subdir in sub_dirs:\n",
    "\n",
    "    print(f\"{subdir}\".center(80, '-'))\n",
    "    proclist = listdir(join(proc_dir, subdir)) # and in proc directory\n",
    "\n",
    "    for file in proclist:\n",
    "\n",
    "        # if we are testing, only apply step to one file\n",
    "        if testing and number_of_preprocessed_files > 0:\n",
    "            continue\n",
    "\n",
    "        # find out subject id (4 digits) and condition (T1, T2, T3, T4) from file name\n",
    "        match = re.search(\"NAP_(?P<subj>\\\\d{4})_(?P<cond>T\\\\d{1})-rpacd.fif\", file)\n",
    "        if not match:\n",
    "            continue\n",
    "        subj = match.group('subj')\n",
    "        cond = match.group('cond')\n",
    "        print(f\"Processing {subj} {cond}\")\n",
    "\n",
    "        # set the file name of the output .fif file and check if it already exists in proc_dir\n",
    "        outfile =  f\"NAP_{subj}_{cond}-rpacda.fif\"\n",
    "        if outfile in proclist and not overwrite:\n",
    "            print(f\"{outfile} already exists in processing dir. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        number_of_preprocessed_files += 1\n",
    "\n",
    "        # now do the actual processing step\n",
    "        # ---------------------------------\n",
    "\n",
    "        try:\n",
    "            raw = mne.io.Raw(join(proc_dir, subdir, file), preload=True)\n",
    "        except:\n",
    "            print(f\"Error loading raw for {subj} {cond}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # get data intervals by the newly added annotations\n",
    "        intervals = intervals_from_annotations(raw, ['Pre_Stimulation','Post_Stimulation.*'], end_interval=True)\n",
    "\n",
    "        # apply artifact annotation\n",
    "        for start, stop, desc in intervals:\n",
    "\n",
    "            old_annotations= raw.annotations.copy()\n",
    "            new_annotations= []\n",
    "            new_annotations.append(annot_grad(raw, start = start, stop = stop))\n",
    "            new_annotations.append(annot_abs(raw, start = start, stop = stop))\n",
    "            new_annotations.append(annot_abs(raw, thresh=15e-5, channel = ['Mov'], start = start, stop = stop))\n",
    "\n",
    "            for annot in new_annotations:\n",
    "                if annot:\n",
    "                    old_annotations = old_annotations.__add__(annot)\n",
    "                    print(annot)\n",
    "            raw.set_annotations(old_annotations)\n",
    "\n",
    "        raw.save(join(proc_dir, subdir, outfile), overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg_locate",
   "language": "python",
   "name": "ieeg_locate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
