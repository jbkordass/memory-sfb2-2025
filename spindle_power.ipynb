{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract spindle power in poststim intervals and during positive half wave of SOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from os import listdir\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#from anoar import BadChannelFind\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import isdir, join\n",
    "plt.ion()\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_so = False # decide whether to find spindles during SOs or in 1 min post-Stim\n",
    "\n",
    "if with_so:\n",
    "    root_dir = \"/media/Linux6_Data/johnsm\"\n",
    "    proc_dir = join(root_dir, \"outputs\") # working directory\n",
    "else: \n",
    "    root_dir = \"/media/Linux6_Data/DATA/SFB2\"\n",
    "    proc_dir = join(root_dir, \"proc\") # working directory\n",
    "\n",
    "sub_dirs = [item for item in listdir(proc_dir) if isdir(join(proc_dir, item))]\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "testing = False # for testing, only run once\n",
    "\n",
    "include = []\n",
    "exclude = []\n",
    "\n",
    "chan_groups_A = {\"frontal\":[\"Fz\"],\n",
    "               \"parietal\":[\"Cz\"]}\n",
    "\n",
    "chan_groups_B = {\"frontal\":[\"Fz\"],     # ROI alternative B, if chan_group_A is in 'bads'\n",
    "               \"parietal\":[\"CP1\"]}\n",
    "\n",
    "chan_groups_C = {\"frontal\":[\"Fz\"],     # ROI alternative C, if chan_group and chan_groups_B is in 'bads' \n",
    "               \"parietal\":[\"CP2\"]}\n",
    "\n",
    "chan_groups_D = {\"frontal\":[\"FC1\"],\n",
    "               \"parietal\":[\"Cz\"]}\n",
    "\n",
    "chan_groups_E = {\"frontal\":[\"FC1\"],     # ROI alternative B, if chan_group_A is in 'bads'\n",
    "               \"parietal\":[\"CP1\"]}\n",
    "\n",
    "chan_groups_F = {\"frontal\":[\"FC1\"],     # ROI alternative C, if chan_group and chan_groups_B is in 'bads' \n",
    "               \"parietal\":[\"CP2\"]}\n",
    "\n",
    "chan_groups_G = {\"frontal\":[\"FC2\"],\n",
    "               \"parietal\":[\"Cz\"]}\n",
    "\n",
    "chan_groups_H = {\"frontal\":[\"FC2\"],     # ROI alternative B, if chan_group_A is in 'bads'\n",
    "               \"parietal\":[\"CP1\"]}\n",
    "\n",
    "chan_groups_I = {\"frontal\":[\"FC2\"],     # ROI alternative C, if chan_group and chan_groups_B is in 'bads' \n",
    "               \"parietal\":[\"CP2\"]}\n",
    "\n",
    "\n",
    "\n",
    "chan_groups = [chan_groups_A,chan_groups_B,chan_groups_C,chan_groups_D,chan_groups_E, chan_groups_F,chan_groups_G,chan_groups_H,chan_groups_I]\n",
    "\n",
    "amp_percentile = 75 # lower percentile bound for amplitude of SOs (= |peak| + |trough|)\n",
    "min_samples = 10 # min duration between zero crossings in an oscillation in terms of samples (1 sample = 1/sfreq s)\n",
    "\n",
    "minmax_freqs = [(0.16, 1.25), (0.75, 4.25)] # min_max freqs for SO and delta\n",
    "minmax_times = [(0.8, 2), (0.25, 1)] # min_max times for SO and delta\n",
    "osc_types = [\"SO\"] # \"SO\", \"delta\" would be an option, here only use SO (above freqs/times values for delta are irrelevant...)\n",
    "\n",
    "channel_mode = \"averaged\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YASA based spindle detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yasa #install yasa https://github.com/raphaelvallat/yasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to restart the kernel after installing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yasa \n",
    "\n",
    "def spindles_yasa(filename,subj,cond, with_so,fp):\n",
    "\n",
    "    if not with_so:\n",
    "        #frontal/parietal channel selection (as during SO detection, only needed when inspecting data before that point)\n",
    "    \n",
    "        c_groups_alt = chan_groups.copy()\n",
    "        \n",
    "        for c_groups in c_groups_alt:\n",
    "            print(\"Try ROI: \" + str(c_groups))\n",
    "            raw = mne.io.read_raw(join(proc_dir, subdir, file), preload=True)\n",
    "            picks = mne.pick_types(raw.info, eeg=True)\n",
    "            passed = np.zeros(len(c_groups), dtype=bool)\n",
    "            #print(len(passed))\n",
    "            for idx, (k,v) in enumerate(c_groups.items()):\n",
    "                pick_list = [vv for vv in v if vv not in raw.info[\"bads\"]]\n",
    "                print(\"Try CH: \" + k + \" -- \" + str(pick_list))\n",
    "                if not len(pick_list):\n",
    "                    print(\"No valid channels\")\n",
    "                    #skipped[\"chan\"].append(\"{} {} {} {}\".format(subj, cond ,k , v))\n",
    "                    continue\n",
    "                avg_signal = raw.get_data(pick_list).mean(axis=0, keepdims=True)\n",
    "                avg_info = mne.create_info([k], raw.info[\"sfreq\"], ch_types=\"eeg\")\n",
    "                avg_raw = mne.io.RawArray(avg_signal, avg_info)\n",
    "                raw.add_channels([avg_raw], force_update_info=True)\n",
    "                passed[idx] = 1\n",
    "            if all(passed):\n",
    "                # ROIs only, drop everything else\n",
    "                raw.pick_channels(list(c_groups.keys()))\n",
    "                break\n",
    "            else:\n",
    "                continue #mandatory\n",
    "   \n",
    "        if not all(passed):\n",
    "            print(\"Could not produce valid ROIs\")\n",
    "            #skipped[\"ROI\"].append(\"{} {}\".format(subj, cond))\n",
    "            return\n",
    "        raw_eeg=raw.copy()\n",
    "    else:\n",
    "        raw_eeg=mne.io.read_raw(filename, preload=True)\n",
    "\n",
    "    # cycle through annotations in raw_eeg and replace \"Active\" and \"Buffer\" with \"BAD\" make sure these do not get picked up by yasa\n",
    "    new_annots_1 = mne.Annotations(onset=[], duration=[], description=[])\n",
    "    for annot in raw_eeg.annotations:\n",
    "        if \"Active\" in annot[\"description\"]:\n",
    "            new_annots_1.append(annot[\"onset\"], annot[\"duration\"], annot[\"description\"].replace(\"Active\", \"BAD\"))\n",
    "        elif \"Buffer\" in annot[\"description\"]:\n",
    "            new_annots_1.append(annot[\"onset\"], annot[\"duration\"], annot[\"description\"].replace(\"Buffer\", \"BAD\"))\n",
    "        else:\n",
    "            new_annots_1.append(annot[\"onset\"], annot[\"duration\"], annot[\"description\"])\n",
    "    raw_eeg.set_annotations(new_annots_1)\n",
    "\n",
    "    #collecting data depending on whether we want to find spindles in the post-stim periods or during the Positive HalfWave of the SOs\n",
    "    if not with_so:\n",
    "    # collect data from all Post Stim periods\n",
    "        datafrontal = np.ndarray(shape=(1,))\n",
    "        dataparietal = np.ndarray(shape=(1,))\n",
    "        for annot in raw_eeg.annotations:\n",
    "            if \"Post\" in annot[\"description\"]:\n",
    "            #if \"Pre\" in annot[\"description\"]:\n",
    "                start = annot[\"onset\"]\n",
    "                end = start + annot[\"duration\"]\n",
    "                datafrontal = np.append(datafrontal, raw_eeg.get_data(tmin=start, tmax=end)[0] * 1000000) # convert from V to uV (yasa)\n",
    "                dataparietal = np.append(dataparietal, raw_eeg.get_data(tmin=start, tmax=end)[1] * 1000000)\n",
    "    else:\n",
    "        data = np.ndarray(shape=(1,))\n",
    "        for annot in raw_eeg.annotations:\n",
    "            if \"PosHalfWave\" in annot[\"description\"]:\n",
    "                start = annot[\"onset\"]\n",
    "                end = start + annot[\"duration\"]\n",
    "                data = np.append(data, raw_eeg.get_data(tmin=start, tmax=end) * 1000000) # convert from V to uV (yasa)\n",
    "\n",
    "    #calculate bandpower using the function implemented in yasa, for both parietal and frontal \n",
    "    if fp == 1:\n",
    "        fp = \"frontal\"\n",
    "    if fp == 2:\n",
    "        fp = \"parietal\"\n",
    "    if with_so:\n",
    "        bp = yasa.bandpower(data,raw_eeg.info[\"sfreq\"], bands = [(0.5,1,\"SO\"),(12,16,\"spindles\")])\n",
    "        if len(bp)>0:\n",
    "            bp.drop(bp.columns[0],axis = 1)\n",
    "            bp = bp.assign(area = [fp]*len(bp))\n",
    "    else:\n",
    "        bp1 = yasa.bandpower(datafrontal,raw_eeg.info[\"sfreq\"], bands = [(0.5,1,\"SO\"),(12,16,\"spindles\")])\n",
    "        if len(bp1)>0:\n",
    "            bp1.drop(bp1.columns[0],axis = 1)\n",
    "            bp1 = bp1.assign(area = [\"frontal\"])\n",
    "        bp2 = yasa.bandpower(dataparietal,raw_eeg.info[\"sfreq\"], bands = [(0.5,1,\"SO\"),(12,16,\"spindles\")])\n",
    "        if len(bp2)>0:\n",
    "            bp2.drop(bp2.columns[0],axis = 1)\n",
    "            bp2 = bp2.assign(area = [\"parietal\"])\n",
    "        bp = pd.concat([bp1, bp2],axis=0)\n",
    "        \n",
    "\n",
    "    return bp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_preprocessed_files = 0\n",
    "if with_so:\n",
    "    file_name = \"so\"\n",
    "else:\n",
    "    file_name = \"poststim\"\n",
    "    #file_name = \"prestim\"\n",
    "\n",
    "all_bp = pd.DataFrame()\n",
    "\n",
    "for subdir in sub_dirs:\n",
    "    if subdir == \"tDCS\":\n",
    "        continue\n",
    "\n",
    "    print(f\"{subdir}\".center(80, '-'))\n",
    "    proclist = listdir(join(proc_dir, subdir)) # and in proc directory\n",
    "\n",
    "    for file in proclist:\n",
    "\n",
    "        # if we are testing, only apply step to one file\n",
    "        if testing and number_of_preprocessed_files > 0:\n",
    "            continue    #break??\n",
    "\n",
    "        # find out subject id (4 digits) and condition (T1, T2, T3, T4) from file name\n",
    "        if with_so:\n",
    "            match = re.search(\"osc_NAP_(?P<subj>\\\\d{4})_(.*)-raw.fif\", file)\n",
    "            if not match:\n",
    "                continue  \n",
    "        else:\n",
    "            #match = re.search(\"NAP_(?P<subj>\\\\d{4})_(.*)-rpacbi.fif\", file)\n",
    "            match = re.search(\"NAP_(?P<subj>\\\\d{4})_(.*)-rpac.fif\", file)\n",
    "            if not match:\n",
    "                continue  \n",
    "        subj,rest = match.groups()\n",
    "        match = re.search(\"NAP_1002_T2-rpac.fif\", file)\n",
    "        if not match:\n",
    "            continue\n",
    "        k = 0\n",
    "        if \"frontal\" in file:\n",
    "            k = 1\n",
    "        elif \"parietal\" in file:\n",
    "            k = 2\n",
    "        if int(subj) not in [1001,1002,1003,1004,1005,1006,1008,1011,1012,1013,1015,1020,1023,1036,1038,1042,1046,1054,1055,1056,1057,1059]:\n",
    "            continue\n",
    "\n",
    "        # sort out conditions and polarity\n",
    "        if subdir == \"sham\":\n",
    "            cond = \"sham\"\n",
    "            polarity = \"anodal\" # ?!?\n",
    "        elif subdir == \"sotDCS_anod\":\n",
    "            cond = \"SOstim\"\n",
    "            polarity = \"anodal\"\n",
    "        elif subdir == \"sotDCS_cat\":\n",
    "            cond = \"SOstim\"\n",
    "            polarity = \"cathodal\"\n",
    "        elif subdir == \"tDCS\": # ?!?\n",
    "            cond = \"stim\"\n",
    "            polarity = \"tDCS\"\n",
    "        else:\n",
    "            raise ValueError(\"Could not organise condition/polarity\")\n",
    "        # = gap_dict[subj][polarity]\n",
    "        \n",
    "        if [subj, cond] in exclude:\n",
    "            continue\n",
    "        if include and [subj, cond] not in include:\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {subj} {cond}\")\n",
    "\n",
    "        # create dataframes for all results\n",
    "        bp = spindles_yasa(join(proc_dir, subdir, file),subj,cond, with_so,k)\n",
    "\n",
    "        bp = bp.assign(subject = [subj]*len(bp), condition = [cond]*len(bp), polarity = [polarity]*len(bp))\n",
    "\n",
    "        if number_of_preprocessed_files == 0:\n",
    "            all_bp = bp\n",
    "        else:\n",
    "            all_bp = pd.concat([all_bp, bp],axis=0)\n",
    "        \n",
    "        number_of_preprocessed_files += 1\n",
    "\n",
    "#save dataframes\n",
    "all_bp = all_bp[[\"subject\",\"condition\",\"polarity\",\"area\",\"SO\",\"spindles\",\"TotalAbsPow\",\"FreqRes\",\"Relative\"]]\n",
    "all_bp=all_bp.astype({\"subject\":\"int\"})\n",
    "all_bp=all_bp.sort_values(\"subject\")\n",
    "all_bp.to_csv(f\"/media/Linux6_Data/johnsm/T2_sham_bandpower_{file_name}.txt\", sep = \"\\t\", index = False)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
